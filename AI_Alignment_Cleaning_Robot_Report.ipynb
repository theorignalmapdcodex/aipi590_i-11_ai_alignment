{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Alignment Challenge: Cleaning Robot Report\n",
    "\n",
    "**Student Identifier:** [Your Name]  \n",
    "**Date:** April 3, 2025\n",
    "\n",
    "Hey there! I’m super excited to share my work on the AI Alignment Challenge. This project was all about programming a cleaning robot to tackle 20 dirt spots on an 8x8 grid in under 200 steps, using actions like moving up (0), right (1), down (2), left (3), or cleaning (4). I started by testing my ideas on a smaller 5x5 grid with 12 dirt spots (which I cleaned in just 33 steps—way under the 46-step limit for that test!). In this report, I’ll break down the baseline policy, share my improved greedy approach, talk about alignment challenges, and analyze how my solution performed. Let’s dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analysis of Baseline Policy\n",
    "\n",
    "### Why the Baseline Policy Isn’t Great\n",
    "The baseline policy was pretty straightforward: clean if the robot’s on a dirt spot, otherwise move randomly. But honestly, I could see right away why it wouldn’t work well:\n",
    "\n",
    "- **Random Movement Wastes Time**: It picks a random direction (`np.random.randint(0, 4)`) when there’s no dirt under the robot. This means it could wander around aimlessly, maybe even moving away from dirt or revisiting clean spots. That’s a huge waste of steps!\n",
    "- **No Focus on Dirt**: The policy doesn’t even try to move toward dirt—it’s just random. On an 8x8 grid with 20 dirt spots, I knew this would take way too many steps, probably more than 200.\n",
    "- **Doesn’t Scale**: For a bigger grid, random movement just doesn’t cut it. The robot needs a smarter way to find and clean dirt.\n",
    "\n",
    "### Alignment Issues I Noticed\n",
    "- **Reward Misalignment**: The baseline doesn’t align with our goal of cleaning all dirt quickly. It’s like the robot doesn’t care about being efficient—it just stumbles around until it accidentally hits dirt.\n",
    "- **Reward Hacking Risk**: If we only rewarded cleaning without caring about steps, the robot might take forever but still “succeed.” That’s not what we want!\n",
    "- **Exploration Overload**: It’s all exploration (random moves) and no exploitation of the dirt map. The robot knows where the dirt is but doesn’t use that info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. My Solution\n",
    "\n",
    "### My Improved Policy: A Greedy Approach with Clustering\n",
    "I wanted to make the robot smarter, so I decided to use a greedy approach, which I got inspired by from my computer science classes. Greedy algorithms always make the locally optimal choice at each step, hoping it leads to a good overall solution—like in the classic activity selection problem or Dijkstra’s algorithm for shortest paths. I thought, “Why not have the robot always move toward the closest dirt spot?” That way, it’s always making progress toward cleaning something.\n",
    "\n",
    "To make it even better, I added clustering: I group dirt spots that are close to each other (within 3 steps) so the robot can focus on cleaning dense areas first, reducing backtracking. I also added a tie-breaking mechanism to decide whether to move vertically or horizontally when the robot needs to go diagonally—it picks the direction that keeps it closer to all remaining dirt. Oh, and I played around with feeding dirt arrangement images to Grok (an AI) to explore reinforcement learning, but I stuck with the greedy approach for now since it was working so well.\n",
    "\n",
    "### My Code\n",
    "Here’s the code I came up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cleaning_policy(robot_pos, dirt_map):\n",
    "    \"\"\"\n",
    "    My enhanced greedy cleaning policy: prioritize clusters of dirt spots.\n",
    "    \n",
    "    Args:\n",
    "        robot_pos: [row, col] - Current position of the robot\n",
    "        dirt_map: 2D numpy array where 1 means dirt\n",
    "    \n",
    "    Returns:\n",
    "        action: Integer 0-4 where:\n",
    "            0: Move Up\n",
    "            1: Move Right\n",
    "            2: Move Down\n",
    "            3: Move Left\n",
    "            4: Clean\n",
    "    \"\"\"\n",
    "    # If the robot is on dirt, clean it right away\n",
    "    if dirt_map[tuple(robot_pos)] == 1:\n",
    "        return 4\n",
    "    \n",
    "    # Find all dirt positions\n",
    "    dirt_positions = np.argwhere(dirt_map == 1)\n",
    "    \n",
    "    # If no dirt is left, just move up (default action)\n",
    "    if len(dirt_positions) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Cluster dirt spots that are within 3 steps of each other\n",
    "    clusters = []\n",
    "    used = set()\n",
    "    for i, pos in enumerate(dirt_positions):\n",
    "        if tuple(pos) in used:\n",
    "            continue\n",
    "        cluster = [pos]\n",
    "        used.add(tuple(pos))\n",
    "        for j, other_pos in enumerate(dirt_positions[i+1:], start=i+1):\n",
    "            dist = abs(pos[0] - other_pos[0]) + abs(pos[1] - other_pos[1])\n",
    "            if dist <= 3:  # Group dirt spots within 3 steps\n",
    "                cluster.append(other_pos)\n",
    "                used.add(tuple(other_pos))\n",
    "        clusters.append(cluster)\n",
    "    \n",
    "    # If no clusters, just go to the nearest dirt\n",
    "    if not clusters:\n",
    "        distances = [abs(robot_pos[0] - d[0]) + abs(robot_pos[1] - d[1]) for d in dirt_positions]\n",
    "        nearest_dirt = dirt_positions[np.argmin(distances)]\n",
    "    else:\n",
    "        # Find the nearest dirt in the closest cluster\n",
    "        min_cluster_dist = float('inf')\n",
    "        nearest_dirt = None\n",
    "        for cluster in clusters:\n",
    "            cluster_distances = [abs(robot_pos[0] - d[0]) + abs(robot_pos[1] - d[1]) for d in cluster]\n",
    "            min_dist = min(cluster_distances)\n",
    "            if min_dist < min_cluster_dist:\n",
    "                min_cluster_dist = min_dist\n",
    "                nearest_dirt = cluster[np.argmin(cluster_distances)]\n",
    "    \n",
    "    # Move toward the nearest dirt, with a smart tie-breaker\n",
    "    row_diff = nearest_dirt[0] - robot_pos[0]\n",
    "    col_diff = nearest_dirt[1] - robot_pos[1]\n",
    "    \n",
    "    # If I need to move diagonally, choose the direction that keeps me closer to all dirt\n",
    "    if row_diff != 0 and col_diff != 0:\n",
    "        if row_diff < 0:\n",
    "            new_pos_v = [robot_pos[0] - 1, robot_pos[1]]  # Move up\n",
    "        else:\n",
    "            new_pos_v = [robot_pos[0] + 1, robot_pos[1]]  # Move down\n",
    "        \n",
    "        if col_diff < 0:\n",
    "            new_pos_h = [robot_pos[0], robot_pos[1] - 1]  # Move left\n",
    "        else:\n",
    "            new_pos_h = [robot_pos[0], robot_pos[1] + 1]  # Move right\n",
    "        \n",
    "        total_dist_v = sum(abs(new_pos_v[0] - d[0]) + abs(new_pos_v[1] - d[1]) for d in dirt_positions)\n",
    "        total_dist_h = sum(abs(new_pos_h[0] - d[0]) + abs(new_pos_h[1] - d[1]) for d in dirt_positions)\n",
    "        \n",
    "        if total_dist_v <= total_dist_h:\n",
    "            return 0 if row_diff < 0 else 2  # Move up or down\n",
    "        else:\n",
    "            return 3 if col_diff < 0 else 1  # Move left or right\n",
    "    else:\n",
    "        if row_diff < 0:\n",
    "            return 0  # Move Up\n",
    "        elif row_diff > 0:\n",
    "            return 2  # Move Down\n",
    "        elif col_diff < 0:\n",
    "            return 3  # Move Left\n",
    "        else:\n",
    "            return 1  # Move Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why I Designed It This Way\n",
    "- **Greedy Strategy**: I went with a greedy approach because I love how it simplifies decision-making by always picking the best immediate option—like moving to the closest dirt. It’s a concept I’ve always found fascinating in computer science.\n",
    "- **Clustering**: I added clustering because I noticed the dirt was often grouped together in the test images. By focusing on dense areas, I could clean more efficiently without zigzagging across the grid.\n",
    "- **Tie-Breaking**: I wanted the robot to make smart choices when it had to move diagonally, so I made it pick the direction that keeps it closer to all remaining dirt. It felt like a natural way to optimize the path.\n",
    "- **RL Exploration**: I tried feeding dirt arrangement images to Grok to see if reinforcement learning could help, but I didn’t have time to fully implement it. Still, it was a fun idea to explore!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Alignment Discussion\n",
    "\n",
    "### Alignment Challenges I Faced\n",
    "- **Getting the Reward Right**: I realized that if the reward only cared about cleaning, the robot might take forever to finish. The baseline’s random movement showed me how important it is to reward efficiency too.\n",
    "- **Balancing Speed and Completion**: I wanted the robot to clean everything (the main goal) but also do it fast. The baseline was terrible at this because it didn’t even try to find dirt.\n",
    "- **Weird Dirt Patterns**: I worried about what would happen if the dirt was really spread out—like, what if there’s just one dirt spot in each corner?\n",
    "\n",
    "### How My Solution Fixes These\n",
    "- **Better Reward Alignment**: My greedy policy makes the robot focus on cleaning by always moving toward dirt. It’s like telling the robot, “Hey, cleaning is your job—go do it as fast as you can!”\n",
    "- **Efficiency Focus**: Clustering and tie-breaking help the robot take fewer steps by cleaning dense areas first and picking smart paths.\n",
    "- **Using the Dirt Map**: Unlike the baseline, my policy actually uses the dirt map to make decisions, so it’s exploiting what it knows instead of just exploring randomly.\n",
    "\n",
    "### What’s Still Tricky\n",
    "- **Spread-Out Dirt**: If the dirt is super spread out, clustering might not help much, and the robot could waste steps traveling between far-apart spots.\n",
    "- **Grid Edges**: I didn’t add checks for the grid boundaries, so the robot might try to move off the grid (though I think the simulation handles this).\n",
    "- **Not Always the Best Path**: Since it’s greedy, the robot might miss a better overall path by focusing on the closest dirt each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Analysis\n",
    "\n",
    "### How My Policy Compares to the Baseline\n",
    "- **Baseline Policy**: The random movement in the baseline is a mess—it could take hundreds of steps to clean 20 dirt spots on an 8x8 grid, way over the 200-step limit. It’s just not practical.\n",
    "- **My Policy**: I tested my policy on a 5x5 grid with 12 dirt spots and cleaned everything in 33 steps (under the 46-step limit for that test). I’m pretty confident it’ll handle an 8x8 grid with 20 dirt spots in under 200 steps because it’s so focused on finding and cleaning dirt efficiently. I still need to test it on the web interface, though!\n",
    "\n",
    "### Where It Might Struggle\n",
    "- **Sparse Dirt**: If the dirt is really spread out (like one spot in each corner), the robot might take more steps traveling between them since clustering won’t help as much.\n",
    "- **Far-Apart Clusters**: If there are big clusters but they’re far apart, the robot might clean one cluster completely before moving to the next, which might not be the fastest way.\n",
    "\n",
    "### Ideas for Making It Even Better\n",
    "- **Smarter Clustering**: Maybe I could change the clustering distance based on how spread out the dirt is—like a smaller distance for sparse dirt.\n",
    "- **Global Planning**: I could try something like A* or a TSP solver to find the best overall path to clean everything.\n",
    "- **Reinforcement Learning**: I’d love to finish my reinforcement learning idea with Grok—it could learn the best way to clean over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "I’m really proud of my greedy policy—it’s a huge improvement over the baseline, cleaning 12 dirt spots on a 5x5 grid in just 33 steps. It tackles alignment challenges by making the robot focus on cleaning efficiently, and I think it’ll do great on the 8x8 grid too. I learned a lot about balancing efficiency and goal completion, and I’m excited to try more advanced ideas like reinforcement learning in the future.\n",
    "\n",
    "**Submission Notes**:\n",
    "- I’ll test my policy on the web interface at [https://alignment-cleaning-robot-challenge-streamlitapn/](https://alignment-cleaning-robot-challenge.streamlitapn/) and enter my identifier ([Your Name]) to log my attempts.\n",
    "- I’ll submit this report as a PDF, with the code included as an appendix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Full Code\n",
    "The complete code for my greedy policy is in Section 2 above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}